<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>AI local knowledge management | Bear's dream</title><meta name="description" content="AI local knowledge management"><meta name="keywords" content="AI,Knowledge,LLM,Model,Ollama"><meta name="author" content="supersheepbear"><meta name="copyright" content="supersheepbear"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon_1.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="p43hXBn3_dLvbyxUzx0eo5eYmj9OVYOV52pdrhBa2sw"><meta name="baidu-site-verification" content="EXLlfLv2pV"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="AI local knowledge management"><meta name="twitter:description" content="AI local knowledge management"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><meta property="og:type" content="article"><meta property="og:title" content="AI local knowledge management"><meta property="og:url" content="https://yxiong.org/2025/06/08/20-areas/ai/AI%20local%20knowledge%20management/"><meta property="og:site_name" content="Bear's dream"><meta property="og:description" content="AI local knowledge management"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://yxiong.org/2025/06/08/20-areas/ai/AI%20local%20knowledge%20management/"><link rel="prev" title="ai drawio" href="https://yxiong.org/2025/06/08/20-areas/ai/ai%20drawio/"><link rel="next" title="aider cheatsheet" href="https://yxiong.org/2025/06/08/20-areas/ai/aider%20cheatsheet/"><link rel="stylesheet" href="hfttps://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: true,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Bear's dream" type="application/atom+xml">
</head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Bear's dream</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-heartbeat" aria-hidden="true"></i><span> 音乐照片</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/photos/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-coffee"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 学习计划</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200520153141.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">151</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">151</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">19</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-heartbeat" aria-hidden="true"></i><span> 音乐照片</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/photos/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-coffee"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 学习计划</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#AI-Local-Knowledge-Management"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">AI Local Knowledge Management</span></a></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#AI-Local-Knowledge-Management"><span class="toc-number">1.</span> <span class="toc-text">AI Local Knowledge Management</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png)"><div id="post-info"><div id="post-title"><div class="posttitle">AI local knowledge management</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2025-06-08<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2025-06-08</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/20-areas/">20-areas</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/20-areas/ai/">ai</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span><span class="post-meta__separator">|</span><i class="fa fa-comments-o post-meta__icon fa-fw" aria-hidden="true"></i><span>评论数:</span><a href="/2025/06/08/20-areas/ai/AI%20local%20knowledge%20management/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2025/06/08/20-areas/ai/AI%20local%20knowledge%20management/" itemprop="commentCount"></span></a></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="AI-Local-Knowledge-Management"><a href="#AI-Local-Knowledge-Management" class="headerlink" title="AI Local Knowledge Management"></a>AI Local Knowledge Management</h1><p><a href="https://blog.csdn.net/bagell/article/details/145898453" target="_blank" rel="external nofollow noopener noreferrer">https://blog.csdn.net/bagell/article/details/145898453</a></p>
<p>好的，我将详细解释您提供的文本内容，并添加潜在的搜索问题和想法，以帮助用户更深入地理解和应用这些信息。</p>
<p><strong>文章内容详解</strong></p>
<p>这篇文档主要介绍了如何在本地部署大型语言模型 (LLM)，重点是使用 Ollama 这个工具来简化部署过程，并介绍了几个基于 Ollama 的知识库搭建方案。</p>
<ol>
<li><p><strong>背景：本地 LLM 部署的挑战</strong></p>
<ul>
<li><strong>LLM 的本质：</strong> LLM 实际上是包含大量参数的神经网络，例如 DeepSeek-R1 模Yy9Xg4Gm11型，它存储了权重矩阵，并采用了混合专家 (MoE) 架构。</li>
<li><strong>硬件要求：</strong> 运行 LLM 需要强大的计算资源，通常需要行业级别的服务器配置。消费级电脑难以直接运行。</li>
<li><strong>环境配置：</strong> 需要手动配置 CUDA/PyTorch 环境，这些是运行深度学习模型所必需的。CUDA 是英伟达提供的 GPU 加速库，PyTorch 是一个流行的深度学习框架。</li>
<li><strong>代码编写：</strong> 需要编写分布式推理代码，以便在多台机器上并行运行模型，加速推理过程。</li>
<li><strong>资源限制：</strong> 还需要处理量化 (降低模型精度以减少内存占用) 和内存溢出等问题，确保模型能在有限的资源下运行。</li>
</ul>
</li>
<li><p><strong>Ollama：简化本地 LLM 部署</strong></p>
<ul>
<li><strong>Ollama 的作用：</strong> Ollama 旨在简化 LLM 的本地部署，它自动处理了上述的许多复杂问题，例如硬件适配、环境配置、量化和内存管理等。</li>
<li><strong>模型管理：</strong> Ollama 还提供了模型管理功能，方便用户下载、存储和使用不同的 LLM。</li>
<li><strong>推理服务构建：</strong> Ollama 可以帮助用户构建基本的推理服务，方便用户通过 API 调用 LLM。</li>
<li><strong>开源项目：</strong> Ollama 是一个开源项目，用户可以在 GitHub 上找到它：<a href="https://github.com/ollama/ollama" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/ollama/ollama</a></li>
<li><strong>项目介绍：</strong> Ollama 的目标是让用户能够快速启动并运行大型语言模型。</li>
<li><strong>安装：</strong> 用户可以从 <a href="https://ollama.com/download" target="_blank" rel="external nofollow noopener noreferrer">https://ollama.com/download</a> 下载 Ollama，并直接安装。安装过程会自动配置环境变量。</li>
</ul>
</li>
<li><p><strong>Ollama 的架构和优化技术</strong></p>
<ul>
<li><strong>C/S 架构：</strong> Ollama 采用客户端-服务器 (C/S) 架构。客户端 (CLI 或桌面应用) 与用户交互，发起模型请求。服务器端负责处理请求、管理模型、加载模型并执行计算。</li>
<li><strong>权重量化：</strong> Ollama 支持 INT8/INT4 量化，可以将模型显存占用降低到原来的 1/2 到 1/4，从而可以在 16GB 内存的设备上运行 65B 参数的模型。</li>
<li><strong>分块加载：</strong> 对于长文本，Ollama 会进行分块处理，避免显存溢出。</li>
<li><strong>GPU/CPU 调度：</strong> Ollama 优先使用 NVIDIA/AMD GPU 进行加速。如果没有 GPU，则通过 Metal 或分布式计算优化 CPU 模式。</li>
</ul>
</li>
<li><p><strong>Ollama 的模型管理机制</strong></p>
<ul>
<li><strong>本地存储：</strong> 模型文件 (blobs 数据) 和元数据 (manifests) 默认存储在 <code>$HOME/.ollama</code> 目录下，支持离线使用。这意味着数据无需上传到云端，适合对隐私敏感的场景。</li>
<li><strong>模型拉取：</strong> 用户可以通过 <code>ollama run &lt;模型名&gt;</code> 命令从远程仓库下载并缓存模型。</li>
<li><strong>模型库：</strong> 用户可以从 <a href="https://ollama.com/library/deepseek-r1" target="_blank" rel="external nofollow noopener noreferrer">https://ollama.com/library/deepseek-r1</a> 选择 DeepSeek-R1 模型的不同参数版本。</li>
</ul>
</li>
<li><p><strong>DeepSeek-R1 蒸馏模型部署</strong></p>
<ul>
<li><strong>蒸馏模型：</strong> 蒸馏模型是通过知识蒸馏技术，将大型模型的知识转移到小型模型中，从而在保持性能的同时降低计算成本。</li>
<li><strong>模型下载：</strong> 运行 <code>ollama run deepseek-r1</code> 命令会自动下载 DeepSeek-R1-Distill-Qwen-7B 模型。</li>
<li><strong>交互：</strong> 下载完成后，用户可以直接在命令行与模型进行交互。</li>
</ul>
</li>
<li><p><strong>Ollama 服务端配置</strong></p>
<ul>
<li><strong>环境变量：</strong> 通过设置环境变量 <code>$env:OLLAMA_HOST=&quot;0.0.0.0&quot;</code>，可以将 Ollama 服务绑定到所有网络接口，方便与其他工具进行交互。</li>
<li><strong>服务启动：</strong> 运行 <code>ollama serve</code> 命令可以启动 Ollama 服务。</li>
<li><strong>常用环境变量：</strong> 文档中列出了一些常用的环境变量，例如 <code>OLLAMA_HOST</code>、<code>OLLAMA_KEEP_ALIVE</code>、<code>OLLAMA_LOAD_TIMEOUT</code>、<code>OLLAMA_MAX_LOADED_MODELS</code>、<code>OLLAMA_MAX_QUEUE</code>、<code>OLLAMA_MODELS</code> 和 <code>OLLAMA_NUM_PARALLEL</code> 等。这些变量可以控制 Ollama 服务的行为。</li>
</ul>
</li>
<li><p><strong>知识库搭建：嵌入模型和 RAG</strong></p>
<ul>
<li><strong>嵌入模型：</strong> 嵌入模型 (例如 BGE-M3) 用于将文本信息转换为计算机可以理解的向量形式，从而方便进行文本分析和操作。</li>
<li><strong>RAG：</strong> 检索增强生成 (RAG) 是一种将外部知识检索与 LLM 相结合的技术。RAG 可以从外部知识源 (例如文档数据库) 中检索相关信息，然后将这些信息与用户的问题一起输入到 LLM 中，从而生成更准确的回答。</li>
<li><strong>注意事项：</strong> 在使用容器部署知识库时，需要注意 Ollama 服务的访问地址。由于容器内的服务无法直接通过 <code>127.0.0.1</code> 或 <code>localhost</code> 访问宿主机，因此需要使用宿主机的 IP 地址 (例如 <code>172.29.176.1:11434</code>)。</li>
</ul>
</li>
<li><p><strong>集成方案：Ragflow、Dify、Cherry Studio、AnythingLLM</strong></p>
<ul>
<li><strong>Ragflow：</strong> Ragflow 是一款基于深度文档理解构建的开源 RAG 引擎。用户可以使用 Docker 部署 Ragflow，并结合 LLM 构建问答系统。</li>
<li><strong>Dify：</strong> Dify 是一个开源的 LLM 应用开发平台。用户可以使用 Docker 部署 Dify，并配置本地模型和嵌入模型，然后导入知识库，创建聊天助手。</li>
<li><strong>Cherry Studio：</strong> Cherry Studio 是一款跨平台的 AI 桌面客户端，支持集成多个 LLM 服务商，包括本地部署的 Ollama。用户可以使用 Cherry Studio 管理知识库，创建 AI 应用。</li>
<li><strong>AnythingLLM：</strong> AnythingLLM 也是一个全栈应用程序，可以结合向量数据库构建私有 ChatGPT。用户可以本地运行 AnythingLLM，并与提供的任何文档进行智能聊天。</li>
</ul>
</li>
<li><p><strong>总结</strong></p>
<ul>
<li>Ragflow相对比较专业，其次是Dify，Cherry Studio和AnythingLLM，但是前两个相对部署较重，后两个客户端，可以直接客户端部署。</li>
</ul>
</li>
</ol>
<p><strong>潜在搜索问题</strong></p>
<p>以下是一些用户可能搜索的问题，这些问题与文章内容相关，可以帮助他们更深入地了解本地 LLM 部署和知识库搭建：</p>
<ul>
<li>Ollama 是什么？它有什么作用？</li>
<li>如何在本地安装和配置 Ollama？</li>
<li>Ollama 支持哪些 LLM 模型？</li>
<li>如何使用 Ollama 运行 DeepSeek-R1 模型？</li>
<li>如何配置 Ollama 的环境变量？</li>
<li>什么是嵌入模型？它在知识库搭建中有什么作用？</li>
<li>什么是 RAG？它如何提高 LLM 的回答质量？</li>
<li>如何使用 Ragflow 搭建知识库？</li>
<li>如何使用 Dify 搭建知识库？</li>
<li>如何使用 Cherry Studio 搭建知识库？</li>
<li>如何使用 AnythingLLM 搭建知识库？</li>
<li>Ollama 的优势和局限是什么？</li>
<li>如何优化 Ollama 的性能？</li>
<li>Ollama 是否支持 GPU 加速？</li>
<li>如何解决 Ollama 部署过程中遇到的问题？</li>
<li>Ollama 适用于哪些场景？</li>
<li>如何将 Ollama 集成到现有的应用中？</li>
<li>本地部署 LLM 的优缺点是什么？</li>
<li>如何选择合适的 LLM 模型？</li>
<li>如何评估 LLM 的性能？</li>
<li>Ollama 的未来发展方向是什么？</li>
<li>大语言模型（LLM）本地部署教程</li>
<li>Ollama安装使用教程</li>
<li>如何利用Ollama构建知识库</li>
<li>本地知识库的解决方案</li>
<li>RAG是什么，如何使用</li>
<li>Dify、Cherry Studio、AnythingLLM有什么区别，应该选择哪一个</li>
</ul>
<p><strong>潜在搜索想法</strong></p>
<p>以下是一些用户可能感兴趣的搜索想法，这些想法可以帮助他们探索本地 LLM 部署的更多可能性：</p>
<ul>
<li>使用 Ollama 搭建一个本地的问答机器人。</li>
<li>使用 Ollama 构建一个私有的代码助手。</li>
<li>使用 Ollama 搭建一个本地的翻译工具。</li>
<li>将 Ollama 集成到自己的项目中，例如智能客服、内容创作等。</li>
<li>探索 Ollama 的更多高级功能，例如模型微调、自定义模型等。</li>
<li>比较 Ollama 与其他本地 LLM 部署工具的优缺点。</li>
<li>研究 Ollama 的源代码，了解其内部实现原理。</li>
<li>参与 Ollama 的开源社区，贡献代码或提出建议。</li>
<li>使用 Ollama 搭建一个本地的知识图谱。</li>
<li>使用 Ollama 构建一个本地的文本摘要工具。</li>
<li>结合Ollama和其他工具，搭建智能家居系统。</li>
<li>利用Ollama，进行本地化的情感分析</li>
<li>用Ollama做本地的文本生成</li>
<li>Ollama与其他LLM部署工具的比较分析</li>
<li>Ollama的模型微调与优化技巧</li>
</ul>
<p><strong>总结</strong></p>
<p>希望这个详细的解释和补充可以帮助你更好地理解和应用本地 LLM 部署的相关知识。</p>
<p>Dify、Cherry Studio、AnythingLLM有什么区别，应该选择哪一个<br>上网查查?</p>
<p>好的，我帮你查阅网络信息，整理 Dify、Cherry Studio 和 AnythingLLM 的区别，并给出一些选择建议。</p>
<p><strong>Dify, Cherry Studio, AnythingLLM 对比分析</strong></p>
<p>这三款工具都是旨在简化 LLM 应用开发和知识库集成的平台，但侧重点和适用场景略有不同：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特性/平台</th>
<th>Dify</th>
<th>Cherry Studio</th>
<th>AnythingLLM</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>定位</strong></td>
<td>LLM 应用开发平台 (Backend Focused)</td>
<td>AI 桌面客户端 (Frontend Focused)</td>
<td>全栈 LLM 应用，私有 ChatGPT 方案</td>
</tr>
<tr>
<td><strong>部署方式</strong></td>
<td>主要基于 Docker 部署，适合服务器环境。</td>
<td>桌面客户端，支持 Windows/macOS/Linux，一键安装。</td>
<td>提供安装包，可本地部署，也可远程托管。</td>
</tr>
<tr>
<td><strong>知识库支持</strong></td>
<td>强大，支持多种数据源 (PDF, 网页等)，提供分段、向量化等高级配置。</td>
<td>支持，可以上传多种文件类型，构建本地数据库，支持 RAG 和来源标注。</td>
<td>支持导入多种文档，添加到工作区。启用后可进行问答，并列出引用文档。</td>
</tr>
<tr>
<td><strong>模型集成</strong></td>
<td>支持本地模型 (Ollama) 和云端模型 API，配置灵活。</td>
<td>支持 300+ 主流 LLM (DeepSeek, OpenAI, Gemini 等)，通过 API 密钥或本地 Ollama 接入。</td>
<td>支持本地模型和云端 API，配置简单。</td>
</tr>
<tr>
<td><strong>主要功能</strong></td>
<td>AI 工作流，RAG 管道，Agent，模型管理，可观测性，快速原型到生产。</td>
<td>多模型集成，知识库管理，预置智能体，多模态处理。</td>
<td>构建私有 ChatGPT，与任何文档智能聊天。</td>
</tr>
<tr>
<td><strong>界面交互</strong></td>
<td>Web 界面，功能丰富，配置项较多，上手需要一定时间。</td>
<td>桌面客户端，界面简洁直观，易于上手。</td>
<td>Web 界面，流程引导清晰。</td>
</tr>
<tr>
<td><strong>开发友好度</strong></td>
<td>平台性质，适合需要定制化开发，构建复杂 AI 应用的场景。</td>
<td>客户端性质，主要面向最终用户，或需要快速构建 AI 应用原型但定制化需求不高的开发者。</td>
<td>全栈应用，适合快速搭建私有 ChatGPT，但定制化能力相对较弱。</td>
</tr>
<tr>
<td><strong>部署复杂度</strong></td>
<td>相对复杂，需要 Docker 基础。</td>
<td>简单，一键安装。</td>
<td>简单，提供安装包。</td>
</tr>
<tr>
<td><strong>社区与生态</strong></td>
<td>活跃的开源社区，持续更新。</td>
<td>较新项目，社区活跃度待观察。</td>
<td>开源项目，有一定社区基础。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>如何选择？</strong></p>
<ul>
<li><strong>选择 Dify 的理由:</strong><ul>
<li><strong>你希望构建一个复杂的、可定制化的 AI 应用</strong>，例如智能客服、知识图谱问答等。</li>
<li><strong>你需要强大的 AI 工作流管理、RAG 管道和模型管理功能</strong>。</li>
<li><strong>你熟悉 Docker 部署</strong>，并希望在服务器端运行应用。</li>
</ul>
</li>
<li><strong>选择 Cherry Studio 的理由:</strong><ul>
<li><strong>你希望快速体验和测试不同的 LLM 模型和知识库</strong>，而不需要复杂的配置。</li>
<li><strong>你更喜欢桌面客户端的交互方式</strong>，并希望在本地电脑上运行 AI 应用。</li>
<li><strong>你需要集成的多模态处理能力</strong>，例如图像生成、代码高亮等。</li>
</ul>
</li>
<li><strong>选择 AnythingLLM 的理由:</strong><ul>
<li><strong>你希望快速搭建一个私有的 ChatGPT</strong>，能够与你的文档进行智能聊天。</li>
<li><strong>你希望能够本地运行或远程托管应用</strong>，并拥有一定的定制化能力。</li>
<li><strong>你寻求一个开箱即用的解决方案</strong>，上手简单。</li>
</ul>
</li>
</ul>
<p><strong>简单总结:</strong></p>
<ul>
<li><strong>Dify:</strong> 后端开发平台，灵活强大，但部署较重。</li>
<li><strong>Cherry Studio:</strong> 桌面客户端，简单易用，适合快速体验。</li>
<li><strong>AnythingLLM:</strong> 全栈应用，开箱即用，适合搭建私有 ChatGPT。</li>
</ul>
<p><strong>建议:</strong></p>
<ul>
<li><strong>如果你是开发者，需要构建复杂的 AI 应用，推荐 Dify。</strong></li>
<li><strong>如果你是普通用户，希望快速体验 LLM 和知识库，推荐 Cherry Studio。</strong></li>
<li><strong>如果你想快速搭建私有 ChatGPT，推荐 AnythingLLM。</strong></li>
</ul>
<p>在选择之前，最好根据你的具体需求和技术背景进行评估，并尝试安装和试用不同的平台，找到最适合你的解决方案。</p>
<p>那如果是rag呢, 加入rag对比这三个 推荐一个</p>
<p>好的，我们聚焦在 RAG (Retrieval-Augmented Generation，检索增强生成) 功能，并以此为核心再次对比 Dify, Cherry Studio, 和 AnythingLLM，并给出推荐。</p>
<p><strong>RAG 功能对比 (重点):</strong></p>
<p>RAG 的核心在于：</p>
<ol>
<li><strong>知识库管理</strong>: 如何摄取、存储、更新和管理外部知识。</li>
<li><strong>检索质量</strong>: 如何高效、准确地检索相关信息。</li>
<li><strong>与 LLM 的集成</strong>: 如何将检索到的信息融入 LLM 的生成过程。</li>
<li><strong>评估与优化</strong>: 如何评估 RAG 效果并进行优化。</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>特性/平台</th>
<th>Dify</th>
<th>Cherry Studio</th>
<th>AnythingLLM</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>知识库管理</strong></td>
<td>非常强大。支持多种数据源 (PDF, TXT, 网页等)，支持分段、向量化 (多种向量数据库可选)，支持知识库版本管理和更新。</td>
<td>支持多种文件类型 (PDF, Word, Excel, 网页链接等)，构建本地数据库。支持向量化处理和来源标注。</td>
<td>支持导入多种文档。可以添加到工作区。支持启用和禁用文档，但高级配置较少。</td>
</tr>
<tr>
<td><strong>检索质量</strong></td>
<td>提供丰富的检索配置选项，例如相似度阈值调整、召回数量控制等。支持自定义 Prompt 优化检索效果。</td>
<td>依赖于内置的检索算法，配置选项相对较少。</td>
<td>依赖于内置的检索算法，配置选项较少。</td>
</tr>
<tr>
<td><strong>LLM 集成</strong></td>
<td>与 LLM 的集成度高。可以灵活地配置 Prompt，控制 LLM 如何利用检索到的信息。支持流式输出。</td>
<td>主要通过预置智能体或自定义 Prompt 与 LLM 集成。</td>
<td>依赖于默认的 Prompt 模板，定制化能力相对较弱。</td>
</tr>
<tr>
<td><strong>评估与优化</strong></td>
<td>提供一定的可观测性，例如可以查看每次问答的检索结果和 LLM 的输出。但缺乏专门的 RAG 评估工具。</td>
<td>缺乏专门的 RAG 评估和优化工具。</td>
<td>缺乏专门的 RAG 评估和优化工具。</td>
</tr>
<tr>
<td><strong>RAG 定制化</strong></td>
<td>RAG 流程高度可定制。可以自定义知识库结构、检索算法、Prompt 模板等。</td>
<td>定制化能力有限。主要通过调整 Prompt 来影响 RAG 效果。</td>
<td>定制化能力较弱。</td>
</tr>
<tr>
<td><strong>易用性</strong></td>
<td>配置项较多，上手需要一定时间。</td>
<td>界面简洁直观，易于上手。</td>
<td>流程引导清晰，上手简单。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>推荐:</strong></p>
<p>基于 RAG 功能的综合考量，我推荐 <strong>Dify</strong>。</p>
<p><strong>原因:</strong></p>
<ul>
<li><strong>最强大的知识库管理</strong>: Dify 提供了最全面的知识库管理功能，支持多种数据源、灵活的分段和向量化策略，以及版本控制和更新机制。这对于构建高质量的 RAG 系统至关重要。</li>
<li><strong>最佳检索质量</strong>: Dify 允许你调整检索参数、自定义 Prompt 以优化检索效果。这意味着你可以根据你的数据和应用场景，精细地控制 RAG 的检索质量。</li>
<li><strong>高度定制化的 RAG 流程</strong>: Dify 允许你自定义 RAG 的各个环节，从知识库结构到检索算法再到 Prompt 模板，都可以根据你的需求进行定制。</li>
<li><strong>平台优势</strong>: Dify 是一个完整的 LLM 应用开发平台，提供了 AI 工作流、Agent、模型管理等功能，可以帮助你构建更复杂的 RAG 应用。</li>
</ul>
<p><strong>其他平台的考量:</strong></p>
<ul>
<li><strong>Cherry Studio:</strong> 如果你只是想快速体验 RAG，并且对定制化要求不高，Cherry Studio 是一个不错的选择。但它的知识库管理和检索功能相对简单。</li>
<li><strong>AnythingLLM:</strong> AnythingLLM 的主要优势在于快速搭建私有 ChatGPT。它的 RAG 功能也比较基础，定制化能力较弱。</li>
</ul>
<p><strong>总结:</strong></p>
<p>如果你对 RAG 有较高要求，需要构建一个高质量、可定制的 RAG 系统，Dify 是最佳选择。 如果你只是想快速体验 RAG，或者需要一个开箱即用的解决方案，Cherry Studio 或 AnythingLLM 也是可以考虑的选项，但需要在 RAG 功能上有所妥协。</p>
<p><strong>建议:</strong></p>
<p>在做最终决定之前，强烈建议你：</p>
<ol>
<li><strong>明确你的 RAG 需求</strong>: 你需要处理哪些类型的数据？你对检索质量和响应速度有什么要求？你需要多高的定制化程度？</li>
<li><strong>试用 Dify, Cherry Studio, 和 AnythingLLM</strong>: 亲自体验这些平台，看看它们是否能够满足你的 RAG 需求。</li>
<li><strong>评估长期成本和维护</strong>: 除了初始部署成本，还要考虑长期维护和升级的成本。</li>
</ol>
<p>希望这个更聚焦于 RAG 的对比分析能帮助你做出更明智的选择。</p>
<p>RAG是什么意思, 解释给我</p>
<p>好的，用通俗易懂的方式解释一下 RAG 是什么：</p>
<p>想象一下，你正在参加一个知识竞赛。</p>
<ul>
<li><p><strong>传统的大语言模型 (LLM) 就像是你自己的大脑</strong>。你的大脑里存储了很多知识，可以回答各种问题。但是，你的大脑的知识是有限的，而且可能会过时。</p>
</li>
<li><p><strong>RAG (Retrieval-Augmented Generation) 就像是你在比赛时可以查阅参考书</strong>。当你遇到一个你不知道的问题时，你可以先查阅参考书，找到相关的资料，然后再根据这些资料来回答问题。</p>
</li>
</ul>
<p><strong>RAG 的全称是 Retrieval-Augmented Generation，翻译成中文是“检索增强生成”。</strong> 它是一种将外部知识检索与大语言模型 (LLM) 相结合的技术。</p>
<p><strong>具体来说，RAG 的工作流程是这样的：</strong></p>
<ol>
<li><strong>Retrieval (检索):</strong> 当用户提出一个问题时，RAG 系统首先从外部知识源 (例如文档数据库、网页、知识图谱等) 中检索与问题相关的资料。这就像你在知识竞赛中查阅参考书。</li>
<li><strong>Augmented (增强):</strong> RAG 系统将检索到的资料与用户的问题一起输入到大语言模型 (LLM) 中。这就像你把参考书中的相关信息与你自己的知识结合起来。</li>
<li><strong>Generation (生成):</strong> LLM 根据用户的问题和检索到的资料，生成最终的答案。这就像你根据参考书中的信息和自己的理解，回答知识竞赛的问题。</li>
</ol>
<p><strong>为什么需要 RAG？</strong></p>
<ul>
<li><strong>解决 LLM 的知识局限性:</strong> LLM 虽然强大，但它的知识是有限的，而且可能会过时。RAG 可以让 LLM 访问最新的外部知识，从而回答更准确、更全面的问题。</li>
<li><strong>提高 LLM 的可信度:</strong> RAG 可以让 LLM 在生成答案时提供知识来源，从而提高答案的可信度。</li>
<li><strong>降低 LLM 的幻觉:</strong> LLM 有时会产生“幻觉”，即生成不真实或不合理的内容。RAG 可以通过检索外部知识来约束 LLM 的生成，从而降低幻觉的发生。</li>
</ul>
<p><strong>RAG 的应用场景：</strong></p>
<ul>
<li><strong>智能客服:</strong> 回答用户关于产品、服务或政策的问题。</li>
<li><strong>知识图谱问答:</strong> 从知识图谱中检索信息，回答用户关于实体、关系或属性的问题。</li>
<li><strong>科研助手:</strong> 帮助研究人员检索文献、分析数据、撰写报告。</li>
<li><strong>内容创作:</strong> 根据外部知识生成文章、博客或新闻报道。</li>
</ul>
<p><strong>总结：</strong></p>
<p>RAG 就像给 LLM 配备了一本“百科全书”，让它在回答问题时可以随时查阅，从而生成更准确、更可信、更全面的答案。它是一种非常有用的技术，可以提高 LLM 在各种应用场景中的表现。</p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI    </a><a class="post-meta__tags" href="/tags/Knowledge/">Knowledge    </a><a class="post-meta__tags" href="/tags/LLM/">LLM    </a><a class="post-meta__tags" href="/tags/Model/">Model    </a><a class="post-meta__tags" href="/tags/Ollama/">Ollama    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2025/06/08/20-areas/ai/ai%20drawio/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>ai drawio</span></div></a></div><div class="next-post pull_right"><a href="/2025/06/08/20-areas/ai/aider%20cheatsheet/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>aider cheatsheet</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2025/06/08/20-areas/ai/MCP vs Function Call vs Agent for AI/" title="MCP vs Function Call vs Agent for AI"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2025-06-08</div><div class="relatedPosts_title">MCP vs Function Call vs Agent for AI</div></div></a></div><div class="relatedPosts_item"><a href="/2025/06/08/20-areas/ai/ai_deepresearch_guide/" title="ai_deepresearch_guide"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2025-06-08</div><div class="relatedPosts_title">ai_deepresearch_guide</div></div></a></div><div class="relatedPosts_item"><a href="/2025/06/08/20-areas/ai/ollama models/" title="ollama models"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2025-06-08</div><div class="relatedPosts_title">ollama models</div></div></a></div><div class="relatedPosts_item"><a href="/2025/06/08/20-areas/ai/Ai prompt enhancing idea/" title="Ai prompt enhancing idea"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2025-06-08</div><div class="relatedPosts_title">Ai prompt enhancing idea</div></div></a></div><div class="relatedPosts_item"><a href="/2025/06/08/20-areas/ai/mega-prompt/" title="mega-prompt"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2025-06-08</div><div class="relatedPosts_title">mega-prompt</div></div></a></div><div class="relatedPosts_item"><a href="/2025/06/08/20-areas/thoughts/Self-cognition_ai_discussion/" title="Self-cognition_ai_discussion"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2025-06-08</div><div class="relatedPosts_title">Self-cognition_ai_discussion</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = true == true ? true : false;
var verify = true == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'xvMxbws16Erd8WUfqxkvHmb5-gzGzoHsz',
  appKey:'4U0Vo7NQIUL0eJzHdkPzdCDr',
  placeholder:'不说话的人会疯掉吗?',
  avatar:'retro',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></main><footer id="footer" style="background-image: url(https://cdn.jsdelivr.net/gh/supersheepbear/blog_source@master/blog/image/20200202103940.png)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2025 By supersheepbear</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="external nofollow noopener noreferrer"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="external nofollow noopener noreferrer"><span>Butterfly</span></a></div><div class="footer_custom_text">外向孤独者患者</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script id="canvas_nest" color="255,240,245" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script src="https://cdn.jsdelivr.net/npm/activate-power-mode/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false; 
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="external nofollow noopener noreferrer" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body></html>